---
title: <center> <h1> "CIND820 - Big Data Analytics Project" </h1> </center>
output: html_document
---
<center> <h1> Prediction of credit card defaulters with machine learning techniques </h1> </center>
<center>  <h3> Dmytro Volynskyy, Ph. D </h2> </center>
<center> <h3> Section D10. Student Number - 501009454 </h2> </center>
---

<style>  
  p {  
    text-indent: 2.0em;  
  }  
  div { text-align: justify; }
</style>


<center> <h2> Initial Dataset Analysis </h2> </center>


```{r, warning=FALSE}
library(ggplot2)
library(mlbench)
library(DataExplorer)
library(dplyr)
library(imbalance)
library(caret)
library(MLeval)
library(ROCR) 
```
      
      
The very first and important step of the machine learning process while performing any data analysis is to prepare data for its proper future interpretation. Clean data can notably increase the accuracy of any model. Usually we start that in R with reading a dataset file using a command ‘read.csv’. We want to have the headers as in original file and do not have factor values. For this ‘header’ argument is set to ‘True’ and ‘stringAsFactors’ – to ‘False’.

```{r}
projectData = read.csv("D:\\Study\\CIND820 - Big Data Analytics Project\\Project\\Initial Analysis\\credit_card.csv", header = TRUE,  sep = ",", stringsAsFactors = FALSE)
```

Now we can start investigating our data. We would need to learn the shape, size, type and general layout of the data that we have. A command ‘head’ shows us the first six rows and gives us understanding how the dataset looks like.

```{r}
head(projectData)
```

There are twenty-five columns describing attributes of each credit card customer in terms of his/her sex, education, age, payment history, bill amount and status of payments in different months (see table 1 from Literature Review). To avoid any complications further, our target variable ‘default.payment.next.month’ is going to be renamed to simpler name 'DEFAULT_PAYMENT' using the following code:

```{r}
colnames(projectData)[colnames(projectData)=="default.payment.next.month"] <- "DEFAULT_PAYMENT"
head(projectData)
```

Then with ‘dim’ function we can easily check that the given data set has 30000 records in it. Using a standard command ‘str’ we may find out of what type is each variable

```{r}
dim(projectData)
str(projectData)
```

Since we have a classification problem in this project, there is a necessity to know the proportion of instances that belong to each class label. This is important because it may highlight an imbalance in the data, that if severe may need to be addressed with rebalancing techniques. The code below creates a useful table showing the number of instances that belong to each class as well as the percentage that this represents from the entire dataset:

```{r}
y <- projectData$DEFAULT_PAYMENT
cbind(freq=table(y), percentage=prop.table(table(y))*100)
```
  
As it is seen, the given dataset has rather imbalanced data where class ‘0’ is presented triple as much as class ‘1’. Due to this we will need to address this issue with specific methods.


<center> <h2> Exploratory Data Analysis </h2> </center>


Applying the summary() function to a data frame will return the summary showing main descriptive statistics (min, 25 percentile, median, mean, 75 percentile, max) for all numeric values. It also indicates, if applicable, the number of missing values for an attribute (marked NA). In this case there no missing values at all.

```{r}
summary(projectData)
```

Comparing details of data description from table 1 and the results of summary() function, it is seen that two attributes, Education and Marriage, have categories either not included in the dataset description or are meaningless.

From the data description received in table 1 we can see that the Education varible has the following categories: 1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown. In addition, we can observe that this attribute holds 0 number as its value, which is not described in the dataset. We may assume that these 0 values along with 5 and 6 can be categorized under value 4 (other).  
A similar situation is for Marriage variable which holds values like 1=married, 2=single, 3 =others. Since the category 0 hasn't been defined anywhere, we will include it in the ‘others’ category marked as 3.

Find distribution of Education and Mariage variables with not described categories in data description. Assigning those to meaningful ones

```{r}
projectData %>% count(EDUCATION, sort = FALSE)
projectData %>% count(MARRIAGE, sort = FALSE)

projectData$EDUCATION[projectData$EDUCATION == 0] <- 4
projectData$EDUCATION[projectData$EDUCATION == 5] <- 4
projectData$EDUCATION[projectData$EDUCATION == 6] <- 4

projectData$MARRIAGE[projectData$MARRIAGE == 0] <- 3

projectData %>% count(EDUCATION, sort = FALSE)
projectData %>% count(MARRIAGE, sort = FALSE)
```

Another important aspect is to explore how the attributes related between each other. For this purpose, the correlation of each pair of numeric attributes will be considered.

```{r fig.align="center", fig.height=9, fig.width=9}
correlations <- cor(projectData[,2:25])
print(correlations)

plot_correlation(na.omit(projectData), maxcat = 5L)
```
    
From this heat map it is seen that particular variables like BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6 have a very weak correlation with a class attribute and thus have no impact on a final outcome. Due to this we will not include them in machine learning models.

Next we create a histogram for each attribute and thus performing a univariate analysis of them:

```{r fig.align="center", fig.height=9, fig.width=9}
plot_histogram(projectData)
```
    
The above graphs show that all PAY variables are skewed to the right. Otherwise there are no specific dependencies to highlight.

<center> <h2> Pre-Processing and Feature Selection </h2> </center>

Next step is to prepare dataset for modelling. As it was mentioned above BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6 attributes have no impact on a class variable and should be deleted from it. That is done with the following code

```{r}
modelData <- select(projectData, !c('ID', 'BILL_AMT1','BILL_AMT2', 'BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6'))
```

Plot a new heat map of correlations after removing attributes from the dataset

```{r, fig.align="center", fig.height=9, fig.width=9}
plot_correlation(na.omit(modelData), maxcat = 5L)
```

It is seen that even after removing specific attributes, all others have not so high correlation with the class variable.

Further we need to deal with class imbalance. For this reason, a CRAN package ‘Imbalance’ is installed. Specifically, a majority weighted minority oversampling technique (MWMOTE) for imbalance dataset will be used. It is a modification for SMOTE technique which overcomes some of the problems of the SMOTE technique when there are noisy instances, in which case SMOTE would generate more noisy instances out of them. Oversampling of the dataset with new instances of class variable '1' using MWMOTE technique is done using the folowing code:

```{r}
newSamples <- mwmote(modelData, numInstances = 10000, classAttr = "DEFAULT_PAYMENT")
summary(newSamples)
```

After creating new artificial records as a separate data frame, we use function ‘rbind’ to merge two datasets

```{r}
newData<-rbind(modelData, newSamples)
```

Then we shuffle it in order not to have only the records of the same outcome class in each fold of the future k-fold cross validation modelling.

```{r}
set.seed(7)
rows <- sample(nrow(newData))
newDataShuffled <- newData[rows, ]

# Transform class variable from numeric to factor

newDataShuffled$DEFAULT_PAYMENT<-factor(newDataShuffled$DEFAULT_PAYMENT, 
                                        labels = c("PayInTime", "Default"))
```

Another correlation heat map is created to see how new instances in a dataset affected dependencies between each other.

```{r, fig.align="center", fig.height=9, fig.width=9}
plot_correlation(na.omit(newDataShuffled), maxcat = 5L)
```
    
It is vividly seen that the values for PAY variables decreased and PAY_AMT increased a little bit in comparison with previous heat map. Nevertheless, we assume that this change is not critical and will not biased the model itself very much.

Then we check again in what proportion the records with different class values are presented in a resulting dataset

```{r}
z <- newData$DEFAULT_PAYMENT
cbind(freq=table(z), percentage=prop.table(table(z))*100)
```

There is a ratio of 58.41% to 41.59% of instances that belong to class ‘0’ and class ‘1’ respectively. Due to this we assume that our dataset is balanced now and we can proceed to the machine learning modelling.

<center> <h2> Model Development </h2> </center>

First of all we split our dataset into training one and validation one with a proportion 70% and 30% repspectively.

```{r}
set.seed(5)
train_index <- sample(1:nrow(newDataShuffled), 0.7 * nrow(newDataShuffled))
train.set <- newDataShuffled[train_index,]
test.set  <- newDataShuffled[-train_index,]
```

We will use a 'caret' package for development of models and we start from defining a trControl argument. The method is set to cross-validation with K equal to 10, classProbs - to TRUE. It is required to return probability of outcome. In this case it is 'default' or 'not'. And summaryFunction is set to return outcome as a set of binary classification results.

```{r}
trainControl <- trainControl(method = "cv", number = 10, classProbs= TRUE,
                              summaryFunction=twoClassSummary, savePredictions = TRUE)
```

Further we are going to develop four models using two machine learning classification algorithms particulary Random Forest and Naive Bayes. Two of them will be generated without pre-processing the dataset, other two will be created with data standardization. After developing we print out their performance metrics, plot the fitted results and the importance of variables.

First the models without pre-processing are being developed. As a method in train() function we set 'rf' for the Random Forest and 'nb' for the Naive Bayes algorithms, metric is set to 'ROC' so that the output data is suitable for generating an ROC curve

```{r, warning=FALSE}
set.seed(777) # setting seed to generate a reproducible random sampling 

#Random Forest model

timeStartRF <- proc.time()

modelRF <- train(DEFAULT_PAYMENT ~., data = train.set, method = "rf", 
                 metric = "ROC", type = "Classification", trControl = trainControl)

proc.time() - timeStartRF


# Print model performance metrics along with other details 

print(modelRF)    # prints the model information

plot(modelRF)     # plots the fitted results. The best AUC is achieved with 2 random predictors

plot(varImp(modelRF))   # returns variable importance

```

```{r, warning=FALSE}
# Naive Bayes model

timeStartNB <- proc.time()

modelNB <- train(DEFAULT_PAYMENT ~., data = train.set, method = "nb", 
                 metric = "ROC", type = "Classification", trControl = trainControl)

proc.time() - timeStartNB

# Print model performance metrics along with other details 

print(modelNB)    # prints the model information

plot(modelNB)     # plots the fitted results. 

plot(varImp(modelNB))   # returns variable importance
```

Now we do the same modelling but with setting additional argument preProc=c("center", "scale") in order to perform standardization

```{r, warning=FALSE}
# Random Forest after scaling

timeStartRFsc <- proc.time()

modelRFsc <- train(DEFAULT_PAYMENT ~., data = train.set, method = "rf", preProc=c("center", "scale"), 
               metric = "ROC", type = "Classification", trControl = trainControl)

proc.time() - timeStartRFsc


# Print model performance metrics along with other details 

print(modelRFsc)    # prints the model information

plot(modelRFsc)     # plots the fitted results. 

plot(varImp(modelRFsc))   # returns variable importance
```

```{r, warning=FALSE}
#Naive Bayes after scaling

timeStartNBsc <- proc.time()

modelNBsc <- train(DEFAULT_PAYMENT ~., data = train.set, method = "nb", preProc=c("center", "scale"), 
                 metric = "ROC", type = "Classification", trControl = trainControl)

proc.time() - timeStartNBsc

# Print model performance metrics along with other details 

print(modelNBsc)    # prints the model information

plot(modelNBsc)     # plots the fitted results. 

plot(varImp(modelNBsc))   # returns variable importance
```

<center> <h2> Predictions and Conclusions </h2> </center>

As long as all models are developed, now we proceed to their validation using a respective part of dataset. After each model returns the outcomes, a confusion matrix is created to get the performance metrics of each classifying algorithm

```{r, warning=FALSE}
# Predict using the validation set with Random Forest

defaultPredictionRF <- predict(modelRF, test.set)         # classification outcome required for the confusion matrix # returns classifications

defaultPredRF <- predict(modelRF, test.set, type = "prob")  # predicted data required for ROC curve # returns probabilities

# Create a confusion matrix for the Random forest prediction results

RFmatrix <- confusionMatrix(defaultPredictionRF, test.set$DEFAULT_PAYMENT, mode="prec_recall")
RFmatrix
```

```{r, warning=FALSE}
# Predict using the validation set with Naive Bayes

defaultPredictionNB <- predict(modelNB, test.set)           # classification outcome required for the confusion matrix # returns classifications

defaultPredNB <- predict(modelNB, test.set, type = "prob")  # predicted data required for ROC curve # returns probabilities

# Create a confusion matrix for the Naive Bayes prediction results

NBmatrix <- confusionMatrix(defaultPredictionNB, test.set$DEFAULT_PAYMENT, mode="prec_recall")
NBmatrix
```

```{r, warning=FALSE}
# Predict using the validation set with Random Forest after scaling

defaultPredictionRFsc <- predict(modelRFsc, test.set)         # classification outcome required for the confusion matrix # returns classifications

defaultPredRFsc <- predict(modelRFsc, test.set, type = "prob")  # predicted data required for ROC curve # returns probabilities


# Create a confusion matrix for the Random Forest after scaling prediction results

scRFmatrix <- confusionMatrix(defaultPredictionRFsc, test.set$DEFAULT_PAYMENT, mode="prec_recall")
scRFmatrix

```

```{r, warning=FALSE}
# Predict using the validation set with Naive Bayes after scaling

defaultPredictionNBsc <- predict(modelNBsc, test.set)           # classification outcome required for the confusion matrix # returns classifications

defaultPredNBsc <- predict(modelNBsc, test.set, type = "prob")  # predicted data required for ROC curve # returns probabilities


# Create a confusion matrix for the Naive Bayes after scaling prediction results

scNBmatrix <- confusionMatrix(defaultPredictionNBsc, test.set$DEFAULT_PAYMENT, mode="prec_recall")
scNBmatrix
```

Further we create plots of ROC curves for each method and calculate the area under the curve

```{r fig.align="center", fig.height=7, fig.width=9}
# Random Forest method without scaling

predRF <- prediction(defaultPredRF[,1], test.set$DEFAULT_PAYMENT)

perfRF <- performance(predRF, "tpr", "fpr")

plot(perfRF, main = "ROC curves for Random Forest and Naive Bayes before and after scaling attributes",
     col = 'black', lty = 1, lwd=6)

# Naive Bayes method without scaling

predNB <- prediction(defaultPredNB[,1], test.set$DEFAULT_PAYMENT)

perfNB <- performance(predNB, "tpr", "fpr")

plot(perfNB, add = TRUE, col = "red", lty = 1, lwd=6)


# Random Forest method with scaling

predRFsc <- prediction(defaultPredRFsc[,1], test.set$DEFAULT_PAYMENT)

perfRFsc <- performance(predRFsc, "tpr", "fpr")

plot(perfRFsc, add = TRUE, col = 'green', lty = 1, lwd=4)


# Naive Bayes method with scaling

predNBsc <- prediction(defaultPredNBsc[,1], test.set$DEFAULT_PAYMENT)

perfNBsc <- performance(predNBsc, "tpr", "fpr")

plot(perfNBsc, add = TRUE, col = "blue", lty = 1, lwd=4)

# Add legends to the plot

legend("right", legend = c("Random Forest w/out scaling", "Naive Bayes w/out scaling", 
                           "Random Forest after scaling", "Naive Bayes after scaling"), 
       bty = "n", cex = 1, lty = 1,
       col = c("black", "red", "green", "blue"), lwd = c(2,2,2,2))
```

```{r}
# AUC for each method

aucRFtemp<-performance(predRF, "auc")
aucRF<-round(as.numeric(aucRFtemp@y.values),3)
aucRF

aucNBtemp<-performance(predNB, "auc")
aucNB<-round(as.numeric(aucNBtemp@y.values),3)
aucNB

aucRFsctemp<-performance(predRFsc, "auc")
aucRFsc<-round(as.numeric(aucRFsctemp@y.values),3)
aucRFsc

aucNBsctemp<-performance(predNBsc, "auc")
aucNBsc<-round(as.numeric(aucNBsctemp@y.values),3)
aucNBsc
```

In the end we create a summary table with performance metrics for each developed model in order to compare them and to decide which is the best in our case. For the purpose of comparison of the performance of our models the following parameters from a confusion matrix are going to be used: Accuracy, Precision, Recall, F1 measure and Area under ROC curve (AUC).

```{r}
mlMethods <- c('Random Forest', 'Naive Bayes', 'Random Forest after scaling', 'Naive Bayes after scaling')

accuracyValue <- c(RFmatrix$overall[[1]], NBmatrix$overall[[1]], scRFmatrix$overall[[1]], scNBmatrix$overall[[1]])

precisionValue <- c(RFmatrix$byClass[[5]], NBmatrix$byClass[[5]], scRFmatrix$byClass[[5]], scNBmatrix$byClass[[5]])

recallValue <- c(RFmatrix$byClass[[6]], NBmatrix$byClass[[6]], scRFmatrix$byClass[[6]], scNBmatrix$byClass[[6]])

F1Value <- c(RFmatrix$byClass[[7]], NBmatrix$byClass[[7]], scRFmatrix$byClass[[7]], scNBmatrix$byClass[[7]])

aucValue <-c(aucRF, aucNB, aucRFsc, aucNBsc)

comparisonTable<-data.frame('ML technique'=mlMethods, 'Accuracy'=accuracyValue,
                            'Precision'=precisionValue, 'Recall'=recallValue, 'F1 Measure'=F1Value,
                            'AUC'=aucValue)

names(comparisonTable) <- gsub("\\.", " ", names(comparisonTable))


comparisonTable<-format(comparisonTable, digits=3)

comparisonTable
```

Accuracy represents the percentage of correct predictions. It is calculated by dividing the number of correct predictions by the number of total predictions. Precision shows the fraction of relevant examples (true positives) among all of the instances which were predicted to belong to a certain class. And recall demonstrates the percentage of total relevant results correctly classified by the algorithm. ROC graphs show us true positive rates plotted against false positive rates. The value of the area under the curve refers to the ability of the classifier to correctly classify true or false case of an attribute. The closer the area under the curve is to 100% the better the classifier.

The first thing that we can state looking at the summary table is that pre-processing of data (standardization) merely does not affect the model accuracy and other metrics. The difference between values of each metric is less than 0.5%. On the other hand, it is vividly seen, that the Random Forest model greatly overperform the Naïve Bayes method by all parameters. This concludes the Random Forest is a better classifier for predicting clients who will be in default next month and will not be able to pay the credit card bill.

In comparison to the results described in [2], where the authors after balancing the dataset with the SMOTE technique got much smaller values of the accuracy, recall, precision and area under ROC curve for the Random Forest classifier (Accuracy – 0.77, Precision – 0.48, Recall – 0.43, AUCROC – 0.65), we can say that the Random Forest models developed in this project are rather accurate and reliable for their practical application.


<center> <h2> CONCLUSION </h2> </center>

The objective of this project was to devise an effective classifier for a credit card company to predict clients who can be in default and will not pay their credit card bills. First of all, the data was cleaned with removing undocumented and mislabeled categories of variables. Then it was explored that among 23 attributes for each client six of them do not affect the outcome class. They were BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6. Therefore, they were excluded from the dataset. Secondly, the oversampling technique such as MWMOTE was used to balance the data. Then predictive analysis of data corpus was performed with the help of two classifiers, particularly the Random Forest and the Naïve Bayes for the classification purpose. As a result of their accuracy and other metrics comparison, it was concluded that for this data the Random Forest classifier works best having the highest values of accuracy (0.86) and recall (0.930). Therefore, it can be used by the credit card company to predict results of worthiness of its customers.
In regards of perspective, this work can be improved by adding other machine learning classification techniques for comparison. Moreover, it would be beneficial to use hyperparameters in order to tune each model for getting more accurate results when predicting default customers. Defining a classifier which takes less time and gives more reliable prediction is of great interest as well since running the model on a big dataset may take a long time and can pose a problem of trade-off between performance and time consuming.





